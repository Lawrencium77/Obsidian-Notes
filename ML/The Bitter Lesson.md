
### Main Argument
The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.

The ultimate reason for this is Moore's Law. Most AI research has been conducted as if the computation available were constant but, over a slightly longer time than a typical research project, massively more computation inevitable becomes available.

Seeking an improvement that makes a difference in the shorter term, researchers seek to leverage their human knowledge of the domain. But the only thing that matters in the long run is the leveraging of computation.

These two need not run counter to each other, but in practice they tend to - time spent on one is time not spent on the other. And the human-knowledge approach tends to complicate methods in ways that make them less suited to taking advantage of general methods leveraging compute.

### Examples
**Chess**: The methods that defeated Kasparov in 1997 were based on massive, deep search. Previous researchers had pursued methods that leveraged human understanding of chess, but to no avail.

**Go**: A similar pattern of progress was seen in Go, only delayed by 20 years. We were successful once search was applied at scale. Learning by self play to learn a value function was also important, and further enables massive compute to be brought to the fore.

**Speech recognition**: There was a competition, sponsored by DARPA, in the 1970s. Entrants included special methods that took advantage of human knowledge of words, phonemes, and the human vocal tract. But newer methods that did much more computation, based on HMMs, won.

**Computer Vision**: Modern NNs use only the notion of convolution and certain kinds of invariances.

### Conclusion
Historically, there is a pattern that:

1) AI researchers have tried to build knowledge into their agents;
2) This always helps in the short term;
3) In the long run it plateus and can inhibit further progress;
4) Breakthrough progress eventually arrives by an opposing approach based on scaling computation.

This seems to suggest that the actual contents of minds are tremendously complex. We should stop trying to find simple ways to think about the contents of human intelligence. Instead, we should build in only the meta-methods that can find and capture the arbitrary complexity of nature.

We want AI agents that can discover like we can, not which contain what we have discovered. Building in our discoveries only makes it harder to see how the discovering process can be done.

