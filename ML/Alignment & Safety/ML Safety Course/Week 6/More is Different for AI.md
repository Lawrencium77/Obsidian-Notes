Based upon [this blog](https://bounded-regret.ghost.io/more-is-different-for-ai/).

## Summary
* When thinking about ML risks, there are two common approaches: Engineering and Philosophy.
* Engineering is empirical. It is anchored on current SoTA systems.
* Philosophy considers the limit of very advanced systems. It entertains thought experiments such as the paperclip maximiser. It sounds more "sci-fi like".
* Both approaches agree that misaligned objectives are an important problem in ML systems. Philosophy is more confident that they're a major issue.
* Both agree that out-of-distribution robustness is an important issue. But philosophy is concerned about whether systems can generalise to settings where there is no data.
* Engineering focuses on tasks where current systems don't work well. Philosophy focuses on tasks that have some significant abstract property.
* The author feels that philosophy is underrated by most ML researchers. 
* But philosophy underrates the importance of empirical evidence. 
* Neither approach is fully satisfying, and we have no good single approach.
* The author outlines future posts, which argue that:
	* More is different - bigger systems will be qualitatively different to today's.
	* Thought experiments provide a "third anchor" to discuss ML failures.
	* Thought experiments suggest ML systems will have weird failure modes.
	* Empirical findings generalise surprisingly far. Well-chosen experiments can tell us a lot about future systems.

## Critique
This article does well to raise the distinction between engineering and philosophical approaches to ML Safety. However, in my opinion, the distinction is not as clear cut as the author believes.

Suppose an individual believes the philosophical arguments behind ML safety, as proposed by Yudkowsky, Bostrom, etc. The engineering concerns from ML safety (as described in this article) would seem less significant - they are more focused in the short term, thus considering systems that don't pose an existential risk.
Despite this, there is an argument that the rational approach is to pursue an engineering path - it is all we have, and a solution to AI alignment will likely consist of a technical solution. 

My point is that a "Philosophical approach" to ML safety can motivate a course of action that looks like it is motivated by the "Engineering approach". This is not communicated in the reading.



