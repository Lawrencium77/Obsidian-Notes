Based on Sections 1 - 3 of [this](https://arxiv.org/pdf/2303.16200.pdf#:~:text=Natural%20selection%20may%20be%20a,designs%20to%20be%20selected%20naturally.) paper by Dan Hendrycks.

# Sections 1 & 2

## Summary
Introduction: 

* Two observations suggest influential AI agents may act against human interest: natural selection as a dominant force in AI interest, and evolution by natural selection tends to cause selfish behaviour.
* This could happen without intentional design.

AIs May Become Distored by Darwinian Forces

Overview:

* Although humans oversee AI development, Darwinian forces will influence which AIs succeed.
* An optimistic outcome seems unlikely. The most effective AIs will likely possess undesirable traits like selfishness, deception, and power-seeking.
* Competition incentivizes the adoption of selfish AI agents with weak moral constraints.
* AI generations will pass quickly, allowing Darwinian forces to shape the AI population rapidly.


Preliminaries:
* Darwinian logic applies when there is variation, retention, and differential fitness. A population of AI agents could exhibit all of these.
* The following sections describe how selfish AI agents pose catastrophic risks.

Variation
* Variation means that there is variation in characteristics among individuals.
* AIs will meet this condition since there are likely to be multiple AI agents that differ from one another.

Retention
* Retention means future iterations tend to resemble previous iterations of individuals.
* As long as each generation of AI is influenced by earlier generations, this condition will be met.

Differential Fitness
* Differential Fitness stipulates that different variants have different propagation rates.
* This will be straightforwardly met; some AIs will be more prevalent than others.

Selfish AIs Pose Catastrophic Risks
* There are two key reasons for this:
	* Intelligence undermines control
	* Evolution has led to undesirable outcomes for humans. It is not necessarily good for any species, including AIs.

## Critique
This reading's logic is based upon the Lewontin Conditions. 

I am no biologist, but the Lewontin Conditions seem reductionist. Darwinian evolution is an enormously complex process, and I struggle to believe that there are only three conditions are necessary for it to arise. They also ignore changes in environment that can occur over time (and will surely happen in the context of AI).

I am therefore disappointed that the section of this reading that introduces the Lewontin Conditions fails to make any reference to their shortcomings

# Section 3

## Summary

* * This section argues that natural selection favors selfish AIs over altruistic AIs.

Biological Altruism and Cooperation
* There natural examples of altriuism - where one organism benefits another by reducing its prospects of passing on its genes.
* E.g. vampire bats donate blood to other group memebrs, to ensure they don't starve.
* This can lead to an argument that we AI will be the same.
* The rest of the section examines this argument by considering the different causes of altruism.

Direct and Indirect Reciprocity
* These are two mechanisms that enable cooperation in nature. 
* Direct reciprocity: one individual helps another based upon the expectation that they'll return the favour.
* Indirect reciprocity is based on reputation: is someone is known as helpful, they're more likely to be helped.
* Reciprocity only makes sense when humans can benefit AIs. Once AIs are sufficiently powerful, the cost-benefit ratio doesn't make sense.

Kin and Group Selection
* These are two mechanisms that promote altruism.
* Kin selection argues that an individual is altruistic towards members with whom it shares genetic similarities.
* Group selection suggests that natural selection sometimes acts on groups of organisms as a whole. 
* Humans may suffer if AIs develop tendencies to favour their own kind.
* Also, group selection promotes inter-group viciousness. Unless humans add value to a group of AIs, AI-human groups would fail to compete with groups composed purely of AIs.


Morality and Reason
* Smarter and wiser AIs may be more moral.
* Throughout history, humans have expanded the circle of those who deserve compassion.
* However, AIs automatically becoming more moral assumes that moral claims are objectively true, that they are genuinely good for humans if AIs apply them, and that AIs that know about morality will make decisions upon this basis.
* Betting the future of humanity on these assumptions is a bad idea.

## Critique
In my opinion, this section misses a fairly obvious counterargument to the claim that AIs will be altruistic.

The section frames this argument as: "People see altruism in nature. So they assume that AIs will be the same." The section refutes this claim by examining the mechanisms of altuism and arguing that they need not apply in AIs.

A simpler counterargument would be to point out that whilst there is altruism in nature, there is also extreme selfishness and violence. Including in humans. Nobody has a sufficient understanding of biology to confidently state which outcome will apply to AIs - biological systems are just too complex.

