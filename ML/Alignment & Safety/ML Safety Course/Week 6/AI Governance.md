These are my answers to the questions about [this post](https://forum.effectivealtruism.org/posts/42reWndoTEhFqu6T8/ai-governance-opportunity-and-theory-of-impact).

### Summary

Intro:
* AI governance concerns how humanity should navigate the transition to advanced AI.
* This concerns both contemporary policy and long-term risk.

Contemporary Policy Challenges:
* AI is being deployed in several important areas.
* Policy makers are trying to catch up.
* To advise, an individual needs both policy and technical AI expertise. 
* It's important to work simultaneously across multiple policy areas.

Long-Term Risks and Opportunities:
* These concern existential risks.

Superintelligent Perspective:
* Focus on risks of AI with superhuman cognitive ability.
* Emphasises need for AI alignment.
* An arms-race makes the problem more difficult; we must manage AI competition.
* Another issue is how to distribute control and economic benefit from an artificial superintelligence.

Ecology and GPT Perspectives:
* Considers a diverse set of AI systems.
* GPT perspective regards AI as a GPT. AI could create lots of problems, e.g. increase inequality.

Misuse Risks, Accident Risks, Structural Risks:
* Accidentally using an AI unethically could cause harm.
* This is a concern of the superintelligent perspective. 
* Ecology/GPT perspectives are more concerned with broader structural risks.

Concrete Pathways to Existential Risk:

Nuclear Instability -  Tech improvement could increase the threat of nuclear war.

Power Transitions, Uncertainty, and Turbulence - Tech can cause power transitions, leading to instabiliity.

Inequality, Labor Displacement, Authoritarianism - The world could become more unequal. In the limit, AI could catalyze totalitarianism.

Epistemic Security - Mass manipulation in the political sphere. Advanced democracies could suffer.

Value Erosion through Competition - An arms race could force corporations to cut corners in safety.

Prioritization and Theory of Impact: 
* We must decide an optimal allocation of investments.
* The author argues for a broader portfolio. But with prioritization.
* The field building model of research should be preferred over the product model.

### Nontrivial Strength
The main strength of this article is that it relates longtermist arguments and relates them to contemporary policy. 
Contemporary policy arguments are more likely to fall within the Overton Window.So the author does well to describe how such concerns are related to existential risk. The section "Concrete Pathways to Existential Risk" is particularly strong in this regard: the reader need not believe that superhuman AI will cause existential catastrophe in its own right (through e.g. paperclipping), but instead can understand that powerful AI still is extremely dangerous.