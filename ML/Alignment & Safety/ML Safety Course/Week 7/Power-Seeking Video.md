These notes are based on the first 35 minutes of [this video](https://www.youtube.com/watch?v=UbruBnv3pZU).

* We discuss the case for being worried about AI xrisk.
* Intelligence provides a mechanism for exerting control. Any project that builds a more intelligent system that us should be approached with extreme caution.
* Strategic AI will have instrumental incentives to gain power. 
* There are 6 stages to the argument:
	* Timelines: It will become possible to build dangerous AI
	* Incentives: There will be strong incentives to build such systems
	* Alignment hardness: It's harder to build aligned systems than misaligned systems.
	* High-impact failures: Deployed, misaligned systems will fail in high impact ways.
	* Scaling: All humans will be permanently disempowered.
	* Disempowerment = existential catastrophe.
* There are distinctions between AI and previous technologies we've built: 
	* There are barriers to our understanding of them. 
	* Adversarial dynamics (similar to biorisk)
	* Stakes of error

## Critique
My main criticism of this video is the lack of examples used to reinforce the points made. Whilst I am generally sympathetic to the speakers arguments, there is little evidence used to strengthen their claims.

For example, they provide no examples of difficulties that AI researchers have had thus far in aligning AI systems. Even in April 2021, there was plenty of evidence for this: reward hacking, GPT-3 misalignment, etc. 

I accept that such examples do not exist for all points made during the presentation. For instance, there are no examples of all humans being permanently disempowered by AI! But the argument as a whole could have been strengthened as above.













Lack of examples? GPT-3? 
