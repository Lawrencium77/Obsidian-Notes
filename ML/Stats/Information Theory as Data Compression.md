These are just a few notes to summarise how familiar concepts in information theory can be expressed in terms of data compression. I could definitely understand this topic better!

## Problem Description

Shannon originally defines a **data communication** system as being composed of three elements:

1. A source of data
2. A communication channel
3. A reciever

The "fundamental problem of communication" is for the receiver to be able to identify what data was generated by the source, based on the signal it receives through the channel.

## Relationship to Entropy
A reminder that the entropy of a random variable $x \sim P(X)$ is:

$H(P(X)) = \mathbb{E}_{x\sim P}(I_x(P(x)))$

In Shannon's famous **source coding theorem**, it is proved that **the entropy of  a random variable represents the limit on how well data from the source can be losslessly compressed onto a perfectly noiseless channel**.

By "lossless", we mean that the original message can always be recovered by decompression. This means the compressed message has the same quantity of information as the original, but communicated in fewer characters. It has more information (higher entropy) per character.

Another way of phrasing Shannon's source coding theorem is that a lossless compression scheme cannot compress messages, on average, to have *more* than one bit of information per bit of message.

This can be summarised simply by stating the entropy is the **minimum expected number of bits needed to communicate across a channel**.

## Relationship to Cross Entropy
A reminder that the cross entropy of a distribution $q$ relative to $p$ over a given set is:

$H(p,q)=\mathbb{E}_p(I_X(q))=KL(p||q)+H(p)$

It is now fairly easy to understand that cross entropy is the **average number of bits needed to communicate if a coding scheme is optimised for distribution $q$, rather than true distribution $p$**.

## Relationship to KL Divergence
From the above formula, it's now easy to see that KL divergence is the **expected number of bits that need to be sent in addition to the optimal encoding**.


