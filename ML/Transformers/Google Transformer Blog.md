This is the blog post that complements  the [Attention is All You Need](Attention%20is%20All%20You%20Need.md) paper. 
The post itself can be found here: https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html.

## Intuition for Attention
In my opinion, the main useful bit from this post is that it helps give a bit more of a narrative for what the attention mechanism actually does.
* To compute the next representation for a given word - "bank", for example - the Transformer compares it to every other word in the sequence. 
* The result of these comparisons is an attention score for every other word. These attention scores determine how much each of the other words should contribute to the next representation of "bank".
* The attention scores are then used as weights for a weighted average of all words' representations, which is fed into a fully-connected network to generate a new representation for "bank".

The blog also has a useful animation of how the Transformer can be applied to machine translation. 
* It starts by generating the value vectors - these can be thought of as initial representations (i.e. embeddings) for each word. 
* Then, using self-attention, it aggregates information from all of the other words to generate a new representation per word informed by the entire context. This step is repreated multiple times in parallel for al words, successively generating new representations.
* The decoder operates similarly, but generates one word at a time (from left to right). It attends not only to the other previously generated words, but also to the final representations generated by the encoder.
