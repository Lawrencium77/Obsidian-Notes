```toc
```

## Game Theory
What does optimality mean in a game? Suppose we have a multi-player game - what is the optimal policy $\pi^i$ for the $i$-th player? Clearly, it depends on the policies of other players.

Consider the case where all other players fix their policies, $\pi^{-i}$. The **best response** is defined as the optimal policy against those policies. But this doesn't help us understand what the best policy is overall.

The main game theoretic concept we'll use instead is the **Nash Equilibrium**. This is a joint policy for all players:
$$\pi^i=\pi^i_*(\pi^{-i})$$
such that every player is playing the best response to everyone else. No player would choose to deviate from the Nash policy. We will use this as a gold standard going forwards - if we find a Nash equilibrium, we'll have essentially solved the game.

Best response is the solution to a single-agent RL problem. Other players become part of the environment, and the game is reduced to an MDP. The best response is the optimal policy of this MDP.
The Nash Equilibrium is a fixed-point of **self-play RL**. Experience is generated by playing games between agents

$$a_1 \sim \pi^1, a_2\sim\pi^2,\dots$$
Each agent learns the best response to other players. One player's policy determines another player's environment. If we do find a fixed point, where all players have found an optimal policy, then we've found a Nash Equilibrium.

We will focus on a special class of games: **Two-Player Zero Sum Games**. We assume the two players alternate.
The precise definition of a zero sum game is that it has equal and opposite rewards for black and white:

$$R^1+R^2=0$$

We'll consider methods for finding Nash equilibria in these games. We'll start with game tree search (i.e. planning), and self-play RL (e.g. TD).

We can split games as those which have perfect or imperfect information. A **perfect information aka Markov** game is fully observed (e.g. chess, Go). An **imperfect information** game is partially observed (e.g. chess, poker).

## Minimax Search
A **value function** is defined in the natural way:

![](_attachments/Screenshot%202022-11-24%20at%2016.53.41.png)

We don't need to estimate the value function separately for black and white, since this game is zero sum. This value function tells us who will win.
What we really want to know is what's the *best* way to play? A **minimax value function** maximizes white's expected return while minimizing black's expected return:

![](_attachments/Screenshot%202022-11-24%20at%2016.54.25.png)

This means that both players are trying their best to win the game.
A **minimax policy** is a joint policy $\pi=\langle\pi^1,\pi^2\rangle$ that achieves the minimax values. 
There is a unique minimax value function. A minimax policy is a Nash equilibrium.

Minimax values can be found by depth-first game-tree search. E.g. for tic-tac-toe:

![|400](_attachments/Screenshot%202022-11-24%20at%2017.01.55.png)

More generally, this looks like:

![|500](_attachments/Screenshot%202022-11-24%20at%2017.02.43.png)

This can be computed in a brute-force fashion. We do a depth-first search and once we see a reward, we back up the information. This example is illustrated in the slides in a bit more detail.

Of course, this problem is that the search tree grows **exponentially**. In practice, we use a value function approximator $v(s,\boldsymbol{w})$. We use this to estimate minimax values at leaf nodes. To be clear: we run minimax search to a fixed depth, then use our value function approximator to provide a reward signal which we back up.

## Self-Play Reinforcement Learning
Can we apply value-based RL algorithms to games of self-play? Yes, very easily. We change the definition of the game to be self-play - the return $G_T$ tells us who won the game. 
We can then update our function approximator as follows:

![](_attachments/Screenshot%202022-11-24%20at%2017.15.15.png)

Something which often is different is **Policy Improvement with Afterstates**.
For deterministic games, it is sufficient to estimate $v_*(s)$ because we can evaluate the **afterstate**:

$$q_*(s,a)=v_*(\textrm{succ}(s,a))$$

This is essentially a way to think about the value of actions without having to explicitly reason about $q_\pi(s,a)$. The rules of the game are define by the **successor state** $\textrm{succ}(s,a)$.
Put simply: we consider successor states, evaluate this afterstate, and then pick the afterstate with the highest value. We can do this because the environment is deterministic, and we know the successor function.

To be clear:

![|500](_attachments/Screenshot%202022-11-24%20at%2017.22.02.png)

This improves the joint policy for both players. If we could doing this, we should converge on the minimax values.

> [!INFO]
> I decided to skip the rest of this lecture. It didn't look that useful. The main topic that I'm missing out on is RL in imperfect information games. So might be worth thinking about this in the future.

