```toc
```

# Introduction
To understand exploration-exploitation we will spend most of the lecture considering **multi-armed bandits**. These are a simplified version of the RL problem: you pick one action, get one reward, and that's the end of the episode.

Online decision-making involves a fundamental choice:

* **Exploitation**: Make the best decision given current information.
* **Exploration:** Gather more information.

The best long-term strategy may involve short-term sacrifices. Here are some examples:

![](_attachments/Screenshot%202022-11-20%20at%2013.43.01.png)

To be clear: this is necessary because we are learning **online**. The actions we take affect the data that we see.

We will focus on three different approaches to this problem:

1. Random Exploration: we explore random actions. E.g. $\epsilon$-greedy
2. Optimism in the Face of Uncertainty: This is a general principle. If you're uncertain about the value of an action, you should take that action. You should pick the option with greatest potential.
3. Information State Search: This is the most "correct" but computationally expensive. We consider the agent's information as part of its state. If we bring information into the state description, then we can understand the value in gaining information. 

There is another important distinction to be made here, between **state-action** exploration vs **parameter** exploration. These are two different spaces in which we can explore. 
In state-action exploration, we pick a different action $A$ every time $S$ is visited.
In parameter exploration, we parametrise our policy $\pi_\theta$. To explore, we might change our parameters and see what happens. The main advantage is that we get consistent exploration behaviour. The main disadvantage is that it doesn't know about the state-action space.

# Multi-Armed Bandits
We can think of multi-armed bandits as a simplification of the MDP framework. We discard $\mathcal{P}$ and the state-space:

![](_attachments/Screenshot%202022-11-20%20at%2013.54.41.png)

To be clear: all we have is a set of actions and rewards. The picture on the right illustrates the idea: we have a series of one-armed bandits, each of which has a different payout. We want to maximise how much money we make. We need to balance exploration and exploitation.
Our actions are simply decisions on which machine we use.

## Regret
Let's work through a few definitions:

![](_attachments/Screenshot%202022-11-20%20at%2013.58.15.png)

The action-value is the true payout of a machine. The optimal value is the best we could possibly do (i.e. just picking the machine with maximum payout).

**Maximising our cumulative reward is the same as minimising total regret.**
 
A few more definitions:

![](_attachments/Screenshot%202022-11-20%20at%2014.01.27.png)

The **count** is the number of times we pull an arm. I thin the lecture notes are slightly confusing - I'm not sure if it's the number of times we pull an arm, or the *expected* number of times. In my Ankis, I went for the former.
The **gap** is the difference between the best machine we could have pulled, and a sub-optimal machine.
Our problem is that the gaps, $\Delta_a$, are unknown.

What does regret look like over time? For naive algorithms regret increases for all time:

![](_attachments/Screenshot%202022-11-20%20at%2014.06.34.png)

Is it possible to every achieve sublinear total regret? Yes! But let's first look more closely at our myopic algorithms:

## Greedy and $\epsilon$-greedy algorithms

1. Greedy Algorithm

![](_attachments/Screenshot%202022-11-20%20at%2014.07.50.png)

Can we try to improve on this? The first idea we'll cover is to use **optimistic initialisation**. We start off by assuming the best about all of our actions:

![](_attachments/Screenshot%202022-11-20%20at%2014.12.37.png)

This is the simplest version of optimism in the face of uncertainty. It can work quite well. To be clear: we initialise the values *and* the counts to be optimistic. We assume each bandit has a 100% payout on the basis of some large count, e.g. 100.

2. $\epsilon$-greedy Algorithm

![](_attachments/Screenshot%202022-11-20%20at%2014.08.50.png)

But if we decay our $\epsilon$ over time, it's possible to get sub-linear regret:

![](_attachments/Screenshot%202022-11-20%20at%2014.21.12.png)

To be clear: this requires prior knowledge of $V^*$, i.e. our optimal solution.
What we're now after is an algorithm that achieves logarithmic asymptotic total regret, without knowledge of $V^*$.

## Lower Bound
It turns out that there is a *lower bound* on regret: no algorithm can do better than this lower bound. 
This lower bound depends on the parameters of our problem. Specifically:

![](_attachments/Screenshot%202022-11-20%20at%2014.24.04.png)

The intuition is that an **easy** problem is where one arm is obviously better than all the others. The key point is that the lower bound is **at-least logarithmic in the number of steps**. 

## Upper Confidence Bound
Let us consider the principle of **Optimism in the Face of Uncertainty**:

![](_attachments/Screenshot%202022-11-20%20at%2014.26.50.png)

Imagine there are three arms. This diagram shows our belief on the distribution over $Q$-values. The Optimism in the Face of Uncertainty principle suggests we should take the action with the **highest potential**. In this case, that's the blue arm.

The difficulty is in estimating the uncertainty in these distributions. We'll consider both Bayesian and Frequentist approaches. The general idea we'll use is **Upper Confidence Bounds**:

![](_attachments/Screenshot%202022-11-20%20at%2014.31.28.png)

For $\hat{U}_t(a)$, we use something like a 95% confidence interval. As $N_t(a) \to 0$, $\hat{U}_t(a) \to 0$. 

It is first useful for us to learn about **Hoeffding's Inequality**. This is a fundamental inequality from probability theory:

![](_attachments/Screenshot%202022-11-20%20at%2015.04.03.png)

This applies to **any** distribution. We can apply Hoeffding's Inequality to rewards of the bandit. Conditioned upon selecting action $a$:

![](_attachments/Screenshot%202022-11-20%20at%2015.05.01.png)

We can use this to solve for $U_t(a)$. I.e. it gives us a way to solve for our 95% confidence interval:

![](_attachments/Screenshot%202022-11-20%20at%2015.07.06.png)

We can also add a schedule to out $p$ value. We increase it over time, such that we're more and more sure that we select the optimal action:

![](_attachments/Screenshot%202022-11-20%20at%2015.08.30.png)

This leads to the **UCB1 Algorithm**:

![](_attachments/Screenshot%202022-11-20%20at%2015.09.00.png)

At each step, we pick our actions according to the first formula. And it achieves logarithmic asymptotic total regret.

## Bayesian Bandits
So far, we have made no assumptions about the reward distribution, $\mathcal{R}$ (except that the rewards are bounded). **Bayesian bandits** exploit prior knowledge of rewards, $p[\mathcal{R}]$. They compute a posterior distribution of rewards $p[\mathcal{R}|h_t]$, where $h_t=a_1,r_1,\dots,a_{t-1},r_{t-1}$ is the history.

We then use our posterior to guide exploration, e.g. with Upper Confidence Bounds.
If we have good priors, this is very effective.

How does this work?

![](_attachments/Screenshot%202022-11-20%20at%2015.15.13.png)

As our UCB, we use the mean plus some multiple of standard deviations. 

There is a second way to make use of our probability distribution. This is instead of UCB. It's called **probability matching**:

![](_attachments/Screenshot%202022-11-20%20at%2015.17.19.png)

How is this done in practice? The standard way to implement this is **Thompson Sampling**:

![](_attachments/Screenshot%202022-11-20%20at%2015.20.19.png)

It can be shown that this works optimally well. The idea is very simple: you randomly sample from each of our distributions, and pick the distribution that does best.

## Information State Search
Let's consider our third approach to exploration: Information State Search. 
We should first consider the value of information. If we can quantify this, then we can trade it off perfectly:

![](_attachments/Screenshot%202022-11-20%20at%2015.25.26.png)

We have viewed bandits as **one-step** decision making problems. But we can also view them **sequentially**. We formula this using the **information state**, $\tilde{s}$:

![](_attachments/Screenshot%202022-11-20%20at%2015.26.53.png)

To be clear: we have transformed our bandit problem into an MDP. 
Let's consider the simplest case: the **Bernoulli bandit**. Each bandit is essentially a coin toss.

![](_attachments/Screenshot%202022-11-20%20at%2015.29.35.png)

We have formulated the bandit as an **infinite** MDP over information states. We can solve it by reinforcement learning. 

To make this concrete, let's consider a drug trial. We have two drugs - i.e. two arms we can pull. We begin with some prior distributions over these. We can construct a kind of lookahead tree:

![](_attachments/Screenshot%202022-11-20%20at%2015.33.12.png)

If we solve for this MDP, we get the **optimal** tradeoff between exploration and exploitation. 

# Contextual Bandits
The canonical example of contextual bandits is banner ad placements. The key point is that we have some context; we are told some statistics about each user. This context information becomes a state $s$.

![](_attachments/Screenshot%202022-11-20%20at%2015.42.52.png)

David didn't go over the solutions to this in the lecture; he left it for further reading.

# MDPs
How can we extend these ideas to the full case where we are training some agent?
All the ideas we've seen in the previous sections apply to MDPs:

![](_attachments/Screenshot%202022-11-20%20at%2015.45.32.png)

For instance, applying UCB to MDPs:

![](_attachments/Screenshot%202022-11-20%20at%2015.49.08.png)

# Conclusion

![](_attachments/Screenshot%202022-11-20%20at%2015.51.54.png)


