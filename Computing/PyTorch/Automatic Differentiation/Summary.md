This set of notes was inspired by one of Ed's journal clubs. It contains four sections:

1. A [high-level description of autodiff](What%20is%20Autodiff?.md), based on this [Wikipedia page](https://en.wikipedia.org/wiki/Automatic_differentiation)
2. A [summary](A%20Simple%20Implementation.md) of this [blog](https://sidsite.com/posts/autodiff/) , which contains a Python implementation from scratch
3. [PyTorch Autograd - Basics](PyTorch%20Autograd%20-%20Basics.md), which use this [PyTorch intro](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) blog
4. [PyTorch Autograd - More Detail](PyTorch%20Autograd%20-%20More%20Detail.md), which uses this [more detailed PyTorch page](https://pytorch.org/docs/stable/notes/autograd.html#:~:text=Autograd%20is%20reverse%20automatic%20differentiation,roots%20are%20the%20output%20tensors.).

I also found [this colab notebook](https://colab.research.google.com/drive/1VpeE6UvEPRz9HmsHh1KS0XxXjYu533EC) that has a self-contained implementation of autograd. I may use it at some point.
